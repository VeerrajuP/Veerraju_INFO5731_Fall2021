{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VeerrajuP/Veerraju_INFO5731_Fall2021/blob/master/In_class_exercise_02_resubmission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ANFqefjre_L"
      },
      "source": [
        "## The third In-class-exercise (9/15/2021, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6-6ZvAvre_N"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n8CWkGGre_O"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "VFK84lMxre_O",
        "outputId": "c3eaab48-7dc2-43e7-fbfc-60ab58827429"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nPlease write you answer here:\\n\\nThe research question that i have in mind is the what are effects and consequences of pollution and How to free the world's pollution?\\n\\n\\nThis is my research question. The data should be satelite images of areas where we can identify pollution and chances of pollution in near future.\\n\\n\\nA large dataset is needed for the analysis. The dataset needs to cover the entire globe but we can be specific to certain area or country so we may need less data.\\n\\n\\nThe dataset needs to be satelite images that we can find it from many free open-source websites like kaggle. We may get a huge amount of raw\\n\\n\\n\\n\\n\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "The research question that i have in mind is the what are effects and consequences of pollution and How to free the world's pollution?\n",
        "\n",
        "\n",
        "This is my research question. The data should be satelite images of areas where we can identify pollution and chances of pollution in near future.\n",
        "\n",
        "\n",
        "A large dataset is needed for the analysis. The dataset needs to cover the entire globe but we can be specific to certain area or country so we may need less data.\n",
        "\n",
        "\n",
        "The dataset needs to be satelite images that we can find it from many free open-source websites like kaggle. We may get a huge amount of raw\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7RRwjbbre_P"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 reviews of a movie from IMDB (https://www.imdb.com/) or 1000 reviews of a product from Amazon (https://www.amazon.com/).\n",
        "\n",
        "As for the IMDB movie review, the following informtion need to be collected (for example: https://www.imdb.com/title/tt6751668/reviews?ref_=tt_urv):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time\n",
        "\n",
        "\n",
        "As for the Amazon product review, the following information need to be collected (for example: https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_3?crid=2E3C55VKJX0K3&dchild=1&keywords=machine+learning+andrew+ng&qid=1631718619&sr=8-3):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "def getAmazonSearch(search_query):\n",
        "    url=\"https://www.amazon.com/s?k=\"+search_query\n",
        "    print(url)\n",
        "    page=requests.get(url,headers=header)\n",
        "    if page.status_code==200:\n",
        "        return page\n",
        "    else:\n",
        "        return \"Error\"\n",
        "search_query=\"iphone+10\"\n",
        "base_url=\"https://www.amazon.com/s?k=\"\n",
        "url=base_url+search_query\n",
        "header={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36','referer':'https://www.amazon.com/s?k=nike+shoes+men&crid=28WRS5SFLWWZ6&sprefix=nike%2Caps%2C357&ref=nb_sb_ss_organic-diversity_2_4'}\n",
        "search_response=requests.get(url,headers=header)\n",
        "cookie={} \n",
        "cookie = search_response.cookies\n",
        "product_names=[]\n",
        "response=getAmazonSearch('iphone+10')\n",
        "soup=BeautifulSoup(response.content)\n",
        "for i in soup.findAll(\"span\",{'class':'a-size-base-plus a-color-base a-text-normal'}): \n",
        "    product_names.append(i.text) \n",
        "def Searchasin(asin):\n",
        "    url=\"https://www.amazon.com/dp/\"+asin\n",
        "    print(url)\n",
        "    page=requests.get(url,cookies=cookie,headers=header)\n",
        "    if page.status_code==1000:\n",
        "        return page\n",
        "    else:\n",
        "        return \"Error\"\n",
        "def Searchreviews(review_link):\n",
        "    url=\"https://www.amazon.com\"+review_link\n",
        "    print(url)\n",
        "    page=requests.get(url,cookies=cookie,headers=header)\n",
        "    if page.status_code==1000:\n",
        "        return page\n",
        "    else:\n",
        "        return \"Error\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOe2RCl9uiQV",
        "outputId": "cc7c7c8a-a3b7-4d3c-bde6-e28fb53d4e73"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.amazon.com/s?k=iphone+10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQhk9eeBre_P",
        "outputId": "218aa05d-3de6-4661-ea89-c521cb03998e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.amazon.com/s?k=UnderArmour+shoes+men\n"
          ]
        }
      ],
      "source": [
        "data_asin=[]\n",
        "response=getAmazonSearch('UnderArmour+shoes+men')\n",
        "soup=BeautifulSoup(response.content)\n",
        "for i in soup.findAll(\"div\",class_=\"sg-col-4-of-12 s-result-item s-asin sg-col-4-of-16 AdHolder sg-col sg-col-4-of-20\"):\n",
        "    data_asin.append(i['data-asin'])\n",
        "link=[]\n",
        "for i in range(len(data_asin)):\n",
        "    response=Searchasin(data_asin[i])\n",
        "    soup=BeautifulSoup(response.content)\n",
        "    for i in soup.findAll(\"a\",{'data-hook':\"see-all-reviews-link-foot\"}):\n",
        "        link.append(i['href'])\n",
        "reviews=[]\n",
        "review_title = []\n",
        "review_date = []\n",
        "review_star = []\n",
        "name = []\n",
        "for j in range(len(link)):\n",
        "    for k in range(50):\n",
        "        response=Searchreviews(link[j]+'&pageNumber='+str(k))\n",
        "        soup=BeautifulSoup(response.content)\n",
        "        for i in soup.findAll(\"span\",{'data-hook':\"review-body\"}):\n",
        "            reviews.append(i.text)\n",
        "        for i in soup.findAll(\"span\",{'data-hook':\"review-title\"}):\n",
        "            review_title.append(i.text)\n",
        "        for i in soup.findAll(\"span\",{'data-hook':\"review-date\"}):\n",
        "            review_date.append(i.text)\n",
        "        for i in soup.findAll(\"span\",{'data-hook':\"review-star-rating\"}):\n",
        "            review_star.append(i.text)\n",
        "        for i in soup.findAll('span',class_='a-profile-name'):\n",
        "            name.append(i.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NeSc9ULre_P"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/). \n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1Q2Zkd-33Avg"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.ticker as ticker\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "iterations = 100 \n",
        "\n",
        "def get_data(itr):  \n",
        "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
        "\n",
        "    r = requests.get('https://citeseerx.ist.psu.edu/search?q=web&t=doc&sort=rlv&start='+str(itr), headers=headers)#, proxies=proxies)\n",
        "    content = r.content\n",
        "    soup = BeautifulSoup(content)\n",
        "\n",
        "    alls = []\n",
        "    for d in soup.findAll('div', attrs={'class':'result'}): \n",
        "        all1=[]\n",
        "        title = d.find('a') # to get title\n",
        "        if title is not None:\n",
        "            tt = title.text.strip() \n",
        "            all1.append(tt)\n",
        "        else:\n",
        "            all1.append('Unknown') \n",
        "        venue = d.find('span', attrs={'class':'pubvenue'}) \n",
        "        if venue is not None:\n",
        "            pv = venue.text.strip()\n",
        "            pv= pv.replace(\"-\",\"\") \n",
        "            all1.append(pv)\n",
        "        else:\n",
        "            all1.append('NA') \n",
        "        #print(venue)\n",
        "\n",
        "        year = d.find('span', attrs={'class':'pubyear'}) \n",
        "        if year is not None:\n",
        "            py = year.text.strip()\n",
        "            py= py.replace(\", \",\"\")\n",
        "            #print (py)\n",
        "            all1.append(py)\n",
        "        else:\n",
        "            all1.append('0000') # returns 0000 if there is no year\n",
        "        #print(year)\n",
        "\n",
        "        author = d.find('span', attrs={'class':'authors'}) # to get the authors of book/article\n",
        "        if author is not None:\n",
        "           \n",
        "            pa = author.text.strip()\n",
        "            pa= pa.replace(\"by \\n \",\"\")\n",
        "            all1.append(pa)\n",
        "        else:\n",
        "            all1.append('None')\n",
        "\n",
        "        abstract = d.find('div', attrs={'class':'snippet'}) # to get abstract\n",
        "        if abstract is not None:\n",
        "            ab = abstract.text.strip()\n",
        "            all1.append(ab)\n",
        "        else:\n",
        "            all1.append('Not Available')\n",
        "\n",
        "\n",
        "       \n",
        "        \n",
        "        alls.append(all1)\n",
        "\n",
        "  \n",
        "    return alls\n",
        "\n",
        "results = []\n",
        "for i in range(1, iterations+1):\n",
        "    results.append(get_data(i+10))\n",
        "# as the output will be a list, in order to pass it to the DataFrame it needs to be flattened\n",
        "flatten = lambda a: [item for sublist in a for item in sublist]\n",
        "df = pd.DataFrame(flatten(results),columns=['Title','Venue','Year','Author','Abstract'])\n",
        "df.shape\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "q_OkSTJb3CFs",
        "outputId": "53427ad3-1a60-46a2-b570-78d698bd2f6d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Venue</th>\n",
              "      <th>Year</th>\n",
              "      <th>Author</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rank Aggregation Methods for the Web</td>\n",
              "      <td>NA</td>\n",
              "      <td>2010</td>\n",
              "      <td>Cynthia Dwork,  Ravi Kumar, M...</td>\n",
              "      <td>\"... We consider the problem of combining rank...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data Preparation for Mining World Wide Web Bro...</td>\n",
              "      <td>KNOWLEDGE AND INFORMATION SYSTEMS</td>\n",
              "      <td>1999</td>\n",
              "      <td>Robert Cooley, Bamshad Mobash...</td>\n",
              "      <td>\"... The World Wide Web (WWW) continues to gro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From SHIQ and RDF to OWL: The Making of a Web ...</td>\n",
              "      <td>Journal of Web Semantics</td>\n",
              "      <td>2003</td>\n",
              "      <td>Ian Horrocks,  Peter F. Patel...</td>\n",
              "      <td>\"... The OWL Web Ontology Language is a new fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mfold web server for nucleic acid folding and ...</td>\n",
              "      <td>Nucleic Acids Res</td>\n",
              "      <td>2003</td>\n",
              "      <td>Michael Zuker</td>\n",
              "      <td>\"... The abbreviated name,‘mfold web server’,d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Generating Representative Web Workloads for Ne...</td>\n",
              "      <td>NA</td>\n",
              "      <td>1997</td>\n",
              "      <td>Paul Barford, Mark Crovella</td>\n",
              "      <td>\"... One role for workload generation is as a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>Syskill &amp; Webert: Identifying interesting web ...</td>\n",
              "      <td>In Proc. 13th Natl. Conf. on Artificial Intel...</td>\n",
              "      <td>1998</td>\n",
              "      <td>Michael Pazzani, Jack Muramat...</td>\n",
              "      <td>\"... We describe Syskill &amp; Webert, a software ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Searching The Web: The Public and Their Queries</td>\n",
              "      <td>NA</td>\n",
              "      <td>2001</td>\n",
              "      <td>Amanda Spink,  Deitmar Wolfra...</td>\n",
              "      <td>\"... In studying actual Web searching by the p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Survey of clustering data mining techniques</td>\n",
              "      <td>NA</td>\n",
              "      <td>2002</td>\n",
              "      <td>Pavel Berkhin</td>\n",
              "      <td>\"... Accrue Software, Inc. Clustering is a div...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>httperf - A Tool for Measuring Web Server Perf...</td>\n",
              "      <td>In First Workshop on Internet Server Performance</td>\n",
              "      <td>1998</td>\n",
              "      <td>David Mosberger, Tai Jin</td>\n",
              "      <td>\"... This paper describes httperf, a tool for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>A database and evaluation methodology for opti...</td>\n",
              "      <td>In Proceedings of the IEEE International Conf...</td>\n",
              "      <td>2007</td>\n",
              "      <td>Simon Baker, Daniel Scharstei...</td>\n",
              "      <td>\"... The quantitative evaluation of optical fl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Title  ...                                           Abstract\n",
              "0                 Rank Aggregation Methods for the Web  ...  \"... We consider the problem of combining rank...\n",
              "1    Data Preparation for Mining World Wide Web Bro...  ...  \"... The World Wide Web (WWW) continues to gro...\n",
              "2    From SHIQ and RDF to OWL: The Making of a Web ...  ...  \"... The OWL Web Ontology Language is a new fo...\n",
              "3    Mfold web server for nucleic acid folding and ...  ...  \"... The abbreviated name,‘mfold web server’,d...\n",
              "4    Generating Representative Web Workloads for Ne...  ...  \"... One role for workload generation is as a ...\n",
              "..                                                 ...  ...                                                ...\n",
              "995  Syskill & Webert: Identifying interesting web ...  ...  \"... We describe Syskill & Webert, a software ...\n",
              "996    Searching The Web: The Public and Their Queries  ...  \"... In studying actual Web searching by the p...\n",
              "997        Survey of clustering data mining techniques  ...  \"... Accrue Software, Inc. Clustering is a div...\n",
              "998  httperf - A Tool for Measuring Web Server Perf...  ...  \"... This paper describes httperf, a tool for ...\n",
              "999  A database and evaluation methodology for opti...  ...  \"... The quantitative evaluation of optical fl...\n",
              "\n",
              "[1000 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riip2wtpre_R"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "SQqJqKgwre_R",
        "outputId": "9c402bc3-a37c-4f07-8e77-9ccfd1772fc2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@GMB @GBNEWS @statsjamie @BBCNews @SkyNews @Ju...</td>\n",
              "      <td>2021-12-09 06:07:02</td>\n",
              "      <td>1468824490733981697</td>\n",
              "      <td>Lee Mac</td>\n",
              "      <td>leemmac</td>\n",
              "      <td>737216851</td>\n",
              "      <td>Middlesbrough, England</td>\n",
              "      <td>None</td>\n",
              "      <td>Northerner, military veteran, H&amp;S professional...</td>\n",
              "      <td>False</td>\n",
              "      <td>189</td>\n",
              "      <td>980</td>\n",
              "      <td>11736</td>\n",
              "      <td>14688</td>\n",
              "      <td>2</td>\n",
              "      <td>2012-08-04 18:43:40</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/143571154...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RT @malcolmshabazz6: @cynthiamckinney Studies-...</td>\n",
              "      <td>2021-12-09 06:06:59</td>\n",
              "      <td>1468824478838755331</td>\n",
              "      <td>Hugh</td>\n",
              "      <td>OneFreeBobcat</td>\n",
              "      <td>1395494749</td>\n",
              "      <td>Los Angeles, CA</td>\n",
              "      <td>None</td>\n",
              "      <td>Northern Californian finding my self in LA…aga...</td>\n",
              "      <td>False</td>\n",
              "      <td>344</td>\n",
              "      <td>591</td>\n",
              "      <td>55243</td>\n",
              "      <td>33335</td>\n",
              "      <td>3</td>\n",
              "      <td>2013-05-01 20:15:09</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/936498970...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RT @BarristerSecret: Well how interesting.\\n\\n...</td>\n",
              "      <td>2021-12-09 06:06:55</td>\n",
              "      <td>1468824460287303683</td>\n",
              "      <td>Phil Rees</td>\n",
              "      <td>phrees</td>\n",
              "      <td>15162872</td>\n",
              "      <td>Saltspring Island</td>\n",
              "      <td>https://t.co/tVZ5CX77kK</td>\n",
              "      <td>I write prose, poetry and code.</td>\n",
              "      <td>False</td>\n",
              "      <td>362</td>\n",
              "      <td>1174</td>\n",
              "      <td>3483</td>\n",
              "      <td>3149</td>\n",
              "      <td>16</td>\n",
              "      <td>2008-06-18 21:40:31</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/744309003...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RT @jljcolorado: No wonder the Netherlands is ...</td>\n",
              "      <td>2021-12-09 06:06:54</td>\n",
              "      <td>1468824455942127617</td>\n",
              "      <td>Twitaf</td>\n",
              "      <td>Twitaf3</td>\n",
              "      <td>1435007855732613125</td>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>5</td>\n",
              "      <td>30</td>\n",
              "      <td>1979</td>\n",
              "      <td>1392</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-09-06 22:32:09</td>\n",
              "      <td>https://abs.twimg.com/sticky/default_profile_i...</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@ACTBrigitte #Trump created a monster when he ...</td>\n",
              "      <td>2021-12-09 06:06:46</td>\n",
              "      <td>1468824424925351937</td>\n",
              "      <td>Jack Polakoff</td>\n",
              "      <td>JackPolakoff</td>\n",
              "      <td>4491531140</td>\n",
              "      <td>New York City</td>\n",
              "      <td>https://t.co/SD5Qsu8eAZ</td>\n",
              "      <td>Lifelong liberal Democrat.\\nRetired TV newswri...</td>\n",
              "      <td>False</td>\n",
              "      <td>6834</td>\n",
              "      <td>7301</td>\n",
              "      <td>3293</td>\n",
              "      <td>106749</td>\n",
              "      <td>130</td>\n",
              "      <td>2015-12-08 01:01:40</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/682953082...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>RT @FOX4: The Senate has narrowly approved a r...</td>\n",
              "      <td>2021-12-09 05:34:25</td>\n",
              "      <td>1468816283055570945</td>\n",
              "      <td>The Murder Hornet</td>\n",
              "      <td>dcrysup</td>\n",
              "      <td>341107127</td>\n",
              "      <td>Texas, USA</td>\n",
              "      <td>None</td>\n",
              "      <td>Father. Husband. Civil engineer. Realist. Texa...</td>\n",
              "      <td>False</td>\n",
              "      <td>318</td>\n",
              "      <td>1338</td>\n",
              "      <td>7514</td>\n",
              "      <td>31239</td>\n",
              "      <td>19</td>\n",
              "      <td>2011-07-23 20:28:54</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/142158418...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>New COVID-19 antibody drug OK'd to protect mos...</td>\n",
              "      <td>2021-12-09 05:34:25</td>\n",
              "      <td>1468816282573320197</td>\n",
              "      <td>KSN News Wichita</td>\n",
              "      <td>KSNNews</td>\n",
              "      <td>52099553</td>\n",
              "      <td>Wichita, KS</td>\n",
              "      <td>http://t.co/LmjKkSPTty</td>\n",
              "      <td>Local NBC affilliate</td>\n",
              "      <td>True</td>\n",
              "      <td>73060</td>\n",
              "      <td>1059</td>\n",
              "      <td>3888</td>\n",
              "      <td>123904</td>\n",
              "      <td>693</td>\n",
              "      <td>2009-06-29 16:08:30</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/114935469...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>RT @TigressEllie: England Child Covid Admissio...</td>\n",
              "      <td>2021-12-09 05:34:22</td>\n",
              "      <td>1468816270573412354</td>\n",
              "      <td>Lucy Garrard #SafeEdForAll</td>\n",
              "      <td>LucyGarrard5</td>\n",
              "      <td>1382286035133071370</td>\n",
              "      <td>Hyde, England</td>\n",
              "      <td>None</td>\n",
              "      <td>Mum and Mental Health NHS employee. I love my ...</td>\n",
              "      <td>False</td>\n",
              "      <td>371</td>\n",
              "      <td>533</td>\n",
              "      <td>3791</td>\n",
              "      <td>1832</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-04-14 10:54:27</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/145651962...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>RT @joncoopertweets: I'm sorry but I am STILL ...</td>\n",
              "      <td>2021-12-09 05:34:20</td>\n",
              "      <td>1468816259936661507</td>\n",
              "      <td>Stephen Farris</td>\n",
              "      <td>StephenFarris13</td>\n",
              "      <td>1075890500048220160</td>\n",
              "      <td>Marathon, FL</td>\n",
              "      <td>None</td>\n",
              "      <td>Ellie's husband. High Priesthood. Former Union...</td>\n",
              "      <td>False</td>\n",
              "      <td>16464</td>\n",
              "      <td>16806</td>\n",
              "      <td>115832</td>\n",
              "      <td>74847</td>\n",
              "      <td>9</td>\n",
              "      <td>2018-12-20 23:07:39</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/144569992...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>RT @TomFitton: What is Fauci hiding on gain of...</td>\n",
              "      <td>2021-12-09 05:34:19</td>\n",
              "      <td>1468816258355404802</td>\n",
              "      <td>Live long and prosper 🙏</td>\n",
              "      <td>sunshineday250</td>\n",
              "      <td>4884052625</td>\n",
              "      <td>USA</td>\n",
              "      <td>None</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>2023</td>\n",
              "      <td>4093</td>\n",
              "      <td>58500</td>\n",
              "      <td>77410</td>\n",
              "      <td>70</td>\n",
              "      <td>2016-02-07 06:53:53</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/134771311...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 19 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    0   ...     18\n",
              "0    @GMB @GBNEWS @statsjamie @BBCNews @SkyNews @Ju...  ...  False\n",
              "1    RT @malcolmshabazz6: @cynthiamckinney Studies-...  ...  False\n",
              "2    RT @BarristerSecret: Well how interesting.\\n\\n...  ...  False\n",
              "3    RT @jljcolorado: No wonder the Netherlands is ...  ...   True\n",
              "4    @ACTBrigitte #Trump created a monster when he ...  ...  False\n",
              "..                                                 ...  ...    ...\n",
              "995  RT @FOX4: The Senate has narrowly approved a r...  ...  False\n",
              "996  New COVID-19 antibody drug OK'd to protect mos...  ...  False\n",
              "997  RT @TigressEllie: England Child Covid Admissio...  ...  False\n",
              "998  RT @joncoopertweets: I'm sorry but I am STILL ...  ...  False\n",
              "999  RT @TomFitton: What is Fauci hiding on gain of...  ...  False\n",
              "\n",
              "[1000 rows x 19 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "text_query = 'Coronavirus'\n",
        "max_tweets = 1000\n",
        "consumer_key = \"Sl9QlYe2uIYFB4c56ht3suQpx\"\n",
        "consumer_secret = \"itmSoGGJBHAjnhcy0jZOyBHEEB9oYztj5mLBAa7yJm4tRAb2ai\"\n",
        "access_token = \"1438705493510344707-C3jtWFSrEjfyj0dtBY09lXUVsDL0oi\"\n",
        "access_token_secret = \"XEnNE1Mn6WupA0XnSV8aVNYXrV1k9no4iq6BMDZEOAV5d\"\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "tweets = tweepy.Cursor(api.search,q=text_query,lang='en').items(max_tweets)\n",
        "tweets_list = [[tweet.text, tweet.created_at, tweet.id_str, tweet.user.name, tweet.user.screen_name, tweet.user.id_str, tweet.user.location, tweet.user.url, tweet.user.description, tweet.user.verified, tweet.user.followers_count, tweet.user.friends_count, tweet.user.favourites_count, tweet.user.statuses_count, tweet.user.listed_count, tweet.user.created_at, tweet.user.profile_image_url_https, tweet.user.default_profile, tweet.user.default_profile_image] for tweet in tweets]\n",
        "tweets_df = pd.DataFrame(tweets_list)\n",
        "\n",
        "Tweets = pd.DataFrame()\n",
        "Tweets['User_Name'] = tweets_df[4]\n",
        "Tweets['Posted_Time'] = tweets_df[1]\n",
        "Tweets['Text'] = tweets_df[0]\n",
        "Tweets\n",
        "tweets_df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In-class-exercise-02 (1).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}